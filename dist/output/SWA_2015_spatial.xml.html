<!DOCTYPE html>
<html>
<head>
 <meta charset="UTF-8"/>
 </head>
<body>
<h1>SWA_2015_spatial.xml</h1>
<h1>A supervised machine learning approach for taxonomic relation recognition through non-linear enumerative structures </h1>
<p>Improving relation extraction process requires to have a bet- ter insight of the proper text or to use external resources. Our work lies in the first term of this alternative, and aims at extending works about semantic relation identification in texts for building taxonomies which constitute the backbone of ontologies on which Semantic Web applications are built. We consider a specific discursive structure, the enumera- tive structure, as it bears explicit hierarchical knowledge. This structure is expressed with the help of lexical or typo- dispositional markers whose role is to introduce hierarchical levels between its components. Typo-dispositional markers are unfortunately not integrated into most parsing systems used for information extraction tasks. In order to extend the taxonomic relation identification process, we thus pro- pose a method for recognizing this relation through enu- merative structures which benefit from typo-dispositional markers we called them non-linear enumerative structures . Our method is based on supervised machine learning. Two strategies have been applied a linear classification with a MaxEnt and a non-linear one with a SVM. The results ob- tained in each of these approaches are close, with respec- tively an F1 of 81.25% and of 81.77%. </p>
<h2>Categories and Subject Descriptors </h2>
<p>I.2.7 [Artificial intelligence] Natural language process- ing—Information extraction I.2.6 [Artificial intelligence] </p>
<p>Learning—Knowledge acquisition  relation extraction, taxonomic relation, enumerative struc- ture, text layout, supervised machine learning </p>
<h3>1. INTRODUCTION </h3>
<p>The present work follows on from the ones about seman- tic relation identification in texts for taxonomy construction. Since taxonomies constitute the backbone structure of on- tologies on which Semantic Web applications are often built, lots of papers have been devoted to their construction. The novelty of our study is that it not only takes into account lexico-syntactic markers but also layout features. A written text is not merely a set of words or of sentences. When producing a document, a writer may use, in addi- tion to linguistic skills, the ability to logically and physically structure his her writing. Structuring a text is a complex matter. It could be achieved by various means ranging from strictly linguistics devices such as syntactic arrangement or rhetorical forms to visual typographical or layout choices. Text structuring can serve purposes as diverse as making interpretation easier, organizing complex conceptualization, convincing the reader, etc. It organizes the discourse in a hierarchical way, i.e. as a tree where a child node may be subordinate to its father or is coordinate to its siblings. We will focus in this paper on enumerative structures ES because of their well identified hierarchical structure a primer introduces and confers unity to list of items, the enu- meration. In many cases, the primer and the items express entities that are connected with generic specific relations. These discursive structures may be expressed in a variety of syntactic options they can be contained in a single or in several sentences they may also be expressed in the linearity of the text a or in a bi-dimensional space b . We termed them linear and non-linear ES respectively. </p>
<h3>2. RELATED WORK </h3>
<p>The task of extracting generic specific relation it may also be denoted as taxonomic, hyperonymy, isA or instanceOf relations is critical for ontology construction, enrichment or population. A lot of parameters may affect the type of meth- ods used for this task differences concerning the acquisition context expert or domain documents availability , the genre of texts carefully written or informal , technical properties size of the corpus, coding of documents , the level of pre- cision of the targeted representation thesaurus, lightweight or full-fledged ontology . This task may be carried on using the proper text and or external pre-existing resources. Let us first mention methods based on regular expres- sions known as lexico-syntactic patterns may they be de- fined manually [11, 2], or using a learning process [24]. Such methods benefit from a good precision, but suffer from a poor recall when applied to large texts corpora. Some others methods are based on classification sometimes using super- vised or unsupervised learning. Classification leads to orga- nize terms into classes and sub-classes thanks to syntactic and semantic properties of their uses in texts [9]. Several solutions were proposed varying with the type of selected features part of speech, syntactic dependencies, etc. , with the chosen algorithm SVM, K-Means, etc. , or with the cho- sen similarity measure euclidean distance, cosine similarity, etc. . These methods turn out to present better recall but weaker precision than pattern based approach. Other works use the distributional analysis and identify a semantic proximity between terms frequently occurring in the same contexts. For instance [18] propound a tech- nique based on the distributional inclusion hypothesis to search hyperonymy relations. Another well-known approach is the Formal Concepts Analysis FCA . It consists in the construction of a Galois lattice from contextual information and then to convert it into a concepts hierarchy [8]. Ceder- berg and Widdow [5] combine Latent Semantic Analysis and part-of-speech clues to build taxonomies. Finally, others ap- proaches come from the Information Retrieval area, where relevant terms are extracted from documents and organized into hierarchies [23]. Ontology and thus taxonomy population form text tech- niques are generally based on named entities identification and on the confrontation of them with pre-existing formal resources. For instance, the PANKOW system [7] exploit  patterns and the redundancy of co-occurrences on the web to categorise automatically named entities according to an incomplete ontology. </p>
<h3>3. ENUMERATIVE STRUCTURE </h3>
<p>Enumerating consists in stating the successive equally im- portant elements of a same conceptual domain, these el- ements being hierarchically linked to a classifying concept. This cognitive act is textually realized through a textual ob- ject called enumerative structure ES . As mentioned above, the ES is composed of a primer, of an enumeration intro- duced by the primer and composed of a list of items at least two , and eventually of a conclusion which sums up the various statements given through the items. An ordered relation may exist between items. Hovy and Arens [13] dif- ferentiate the “itematized” list an unordered set from the enumerated list items are “ordered along some underlying dimension, such as time, distance, importance” . This hierarchical structure is expressed with the help of markers whose role is to segment the text each segment corresponding to a component of the ES , and thus to in- troduce hierarchical levels between these components. An ES may combine several of these markers, either lexical or typo-dispositional, and thus can take several forms. As a consequence, the semantic relation born by an ES can be expressed by a variety of markers of different types. </p>
<ul>
<li>The discourse analysis of the ES, according to the Rhetor- ical Structure Theory RST [19] shows a multi-nuclear rela- tion between items and a nuclear-satellite relation between the primer and the enumeration1. The Fig. 1 describes a rhetorical representation of the ES and its corresponding conceptual representation. This analysis confirms that ES </li>
<li>1The theory describes relations between spans of text rather than between the meanings of these texts [22]. </li>
</ul>
<p>are discursive structures, which bear hierarchical knowledge. </p>
<p>Figure 1 Rhetorical and conceptual representations of an enumerative structure. </p>
<p>It should be noted that discourse analysis of enumerative structures is not always quite so simple and so immediate, especially in the context of Natural Language Processing NLP . For instance, the identification of the relation may require background knowledge, as shown in example c . Here items enunciate properties of the“lindy hop”, although no lexical unit present in the primer explicitly expresses that. Moreover, the relation is sometimes difficult to identify, even for an expert, as shown in example d . </p>
<h3>4. DESCRIPTION OF THE CORPUS </h3>
<p>Our corpus originates from the need to enrich the ontol- ogy OntoTopo, built during the GEONTO project2. It is a frame of reference used to localize information relating to urban and environment organizations. One way has been to exploit Wikipedia pages corresponding to the concepts of OntoTopo. Indeed, Wikipedia is a goldmine of information since each page describes one entity by its properties [20]. Furthermore, these properties are often expressed through enumerative structures. We built a first corpus from 168 extracted French Wikipedia pages. From these pages, 745 ES were collected manually. </p>
<p>In a guideline we specified a typology of relations borne by ES. Given our interest in taxonomy learning, we dis- tinguished among the taxonomic relations the isA relations from instanceOf ones as they are used in the ontological do- main . Thus, we defined six classes isA, instanceOf, partOf, otherOntological which gathers non hierarchical relations such as causality, possession, qualities or properties , met- aLinguistic which gathers synonymy, antonomy, etc. rela- tions , and other when the relation cannot be identified . </p>
<ul>
<li>The annotation task has consisted to assign one of these classes to each ES. This task has been performed by three an- </li>
<li>notators two students in linguistics, and an expert in knowl- edge engineering with the annotation tool LARAt Layout Annotation for Relations Acquisition tool 3. The degree of inter-annotator agreement has been evaluated by the sta- tistical index κ [10]. The values per class are reported in table 1 with their z-scores and distributions. We obtained a κ of 0.49 for the whole corpus. This score, considered as correct according to the scale of Landis and Koch [17], reflects the complexity of the annotation task. When the classes isA and instanceOf are merged into a sin- gle taxonomic class, we obtain a κ of 0.54. The adjudication has been based on the majority vote. When the majority failed, the knowledge engineer acted as mediator. </li>
<li>Table 1 Inter-annotator agreements and distribu- tion by classes of relations Classes κ z-score obs. coverage isA 0.45 18.27 268 36.0% instanceOf 0.43 17.40 196 26.3% partOf 0.48 19.53 39 5.2% otherOntological 0.28 11.39 42 5.7% metaLinguistic 0.74 30.40 149 20.0% other 0.23 09.50 51 6.8% Corpus 0.49 36.20 745 100% </li>
<li>This corpus has been pre-processed for the learning tasks. Morphological and syntactic information have been added using the dependency parser Talismane [27]. From this dataset, 599 ES ∼80% were randomly chosen to constitute the de- velopment set dev set , and the remaining 146 ES ∼20% were used for the test set test set . In order to avoid a bias during the evaluation on the test set, these two sets were stratified according to the initial distribution of classes. The whole dataset is available4 and can be used under the terms of the Creative Commons license5. </li>
</ul>
<h3>5. LEARNING TASKS </h3>
<p>In this section, we describe two machine learning algo- rithms, two classification tasks and the feature selection. </p>
<h3>5.1 Machine Learning Algorithms </h3>
<p>Two strategies have been applied for identifying classes a linear classification with a maximum entropy classifier MaxEnt [3] and a non-linear one with a Support Vector Machine SVM [4]. Those algorithms are discriminative and relax the hypothesis of independence in their inputs. </p>
<p>For the SVM, the estimation of the vector Λ is a con- strained quadratic program 4 . The first term of the objec- tive maximizes the margins, and the second one minimizes the errors on the training set with a hinge-loss function ξ. The C determines the cost for the outliers. </p>
<p>Our experiments were run using the OpenNLP6 library for the MaxEnt and the LIBSVM7 implementation for the SVM. In order to avoid overfitting, the hyperparameters were cho- sen on the dev set. A cut-off of 100 is used for the MaxEnt and we apply C 1 and γ 1 p for the SVM. </p>
<h2>5.2 Classification Tasks </h2>
<p>In order to identify the taxonomic relation, we propose two binary classification tasks T Taxo 1 and T Taxo 2. For T Taxo 1, we merge isA and instanceOf classes into a sin- gle class called taxonomic. For T Taxo 2, we also gather the isA and instanceOf classes. However we add the new feature predictOnto resulting from a preliminary binary task T Onto. This latter aims to separate ES into two classes on- tological and non-ontological. The ontological class gathers  isA, instanceOf, partOf and otherOntological classes. </p>
<h3>5.3 Feature Selection </h3>
<p>The features are binary functions that compose the vector x of each ES 5 . Those functions return 1 if a property is verified or 0 otherwise. Given the linguistic nature of the analysis, features evaluate linguistic properties of the words and phrases that form each ES. Because ESs give equal importance to each enumerated element, features are calculated on the primer and the first item only. </p>
<ul>
<li>6http opennlp.apache.org 7http www.csie.ntu.edu.tw ~cjlin libsvm </li>
<li>The selection of features follows an iterative three steps procedure. Firstly a pattern feature is implemented. Sec- ondly the relevance of its instances is evaluated with a Pear- son’s correlation for the two target classes ontological and taxonomic . Finally, the instances with a good correlation are empirically validated on the dev set. We present here the first two steps. The validation on the dev set is presented in Section 6 with the evaluation on the test set. </li>
<li>The patterns which have been applied to the primers are </li>
<li>• f1 x,y,postag [contains primer,postag ] • f2 x,y,postag [ends primer,postag ] • f3 x,y [contains primer,w ∧ isPlural w ] • f4 x,y,A [contains primer,w ∧ isMarker w,A ] • f5 x,y [ends primer,w ∧ isComa w ] • f6 x,y [numberSentences primer v ∧ v 1] • f7 x,y,n [numberTokens primer v ∧ v n] • f8 x,y [ends primer,postag ∧ is postag,V ∨ is postag,ADV ∨ is postag,CS ∨ is postag,PREP ∨ is postag,PRO ] </li>
<li>The patterns which have been applied to the iems are </li>
<li>• f9 x,y,postag [contains item,postag ] • f10 x,y,postag [begins item,postag ] • f11 x,y [numberSentences item v ∧ v 1] • f12 x,y,n [numberTokens item v ∧ v n] • f13 x,y,w [begins Item,w ] </li>
<li>The feature f4 x,y,A verifies if a word or sequence of words w belongs to the alphabet A. The feature f8x,y indicates that items are the complements of a previously in- troduced verb. </li>
<li>The Pearson’s correlation r is calculated as below </li>
<li>cov f, y </li>
<li>rf 6 σ f σ y </li>
<li>where f ∈ Rn is a binary vector where each dimension i is the feature f applied to an ES of the dev set. The vector y ∈ Rn is a binary vector which represents all the labels. The combination of all of the features improves the predic- tion, but some of them give more information for the classifi- cation. The following table Table 2 describes the ten more discriminant features for the ontological class. Most of them are related to the primer, fact which denotes the presence of discriminating markers to identify the ontological class from the non one metaLinguistic and other . </li>
<li>Table 2 ontological class the 10 relevant features </li>
<li>Pattern Instance of feature Segment r corr. f7 numberToken 1 Primer -0.236 f9 postag V Item -0.219 f3 w is plural Primer 0.210 f9 postag PREP Item 0.210 f1 postag V Primer 0.195 f1 postag NPP Primer 0.176 f4 markers of components Primer 0.151 f8 ES with a syntactic hole Primer 0.141 f4 linguistic markers Primer -0.126 f12 numberToken 3 Item 0.099 </li>
<li>The table 3 presents the most discriminant features to identify the taxonomic class. Conversely, we notice here that </li>
</ul>
<p>most of these features are related to the items, as primers does not contain specific markers for the taxonomic relation. </p>
<ul>
<li>Table 3 taxonomic class the 10 relevant features </li>
<li>Pattern Instance of feature Segment r corr. f9 postag V Item -0.259 f13 w ”de” Item 0.235 f12 numberToken 5 Item 0.147 f9 postag NPP Item 0.132 f10 postag N Item 0.128 f3 w is plural Primer 0.120 f1 postag NPP Primer 0.120 f10 postag VINF Item -0.113 f4 linguistic markers Primer -0.112 f12 numberToken 3 Item 0.107 </li>
<li>The feature predictOnto, resulting from the T Onto task and used in T Taxo 2, has the form </li>
<li>f14 x,y [isPredictedAs x,ontological ] </li>
</ul>
<p>For respectively the MaxEnt and the SVM, the rf14 has for value 0.361 and 0.321. Those are the higher values among all the features. </p>
<ul>
<li>Table 4 Evaluation for the ontological class </li>
<li>dev set Prec. Rec. F1 Acc. MaxEnt 82.77 89.95 86.21 78.96 </li>
<li>SVM 80.42 95.66 87.38 79.80 </li>
<li>baseline 73.12 100.0 84.47 73.12 test set Prec. Rec. F1 Acc. MaxEnt 80.65 93.46 86.58 78.77 </li>
<li>SVM 79.84 96.26 87.29 79.45 </li>
<li>baseline 73.28 100.0 84.58 73.28 </li>
<li>The table 5 presents the results for T Taxo 1 and T Taxo 2 tasks the models names for the T Taxo 2 task end with a “+” . The baseline shows a good precision thanks to the distribution 62.03% vs. 37.97% . In both task, the SVM+ show the best F1 for the taxonomic class. </li>
<li>dev set Prec. Rec. F1 Acc. MaxEnt 70.89 81.18 75.69 67.61 </li>
<li>SVM 72.53 88.71 79.81 72.12 </li>
<li>MaxEnt+ 73.03 82.26 77.37 70.12 </li>
<li>SVM+ 73.57 89.78 80.87 73.62 </li>
<li>baseline 62.10 100.0 76.62 62.10 test set Prec. Rec. F1 Acc. MaxEnt 70.59 78.26 74.23 65.75 </li>
<li>SVM 71.05 88.04 78.64 69.86 </li>
<li>MaxEnt+ 78.00 84.78 81.25 75.34 </li>
<li>SVM+ 74.77 90.22 81.77 74.66 </li>
<li>baseline 63.01 100.0 77.31 63.01 </li>
<li>Table 6 Comparison for the taxonomic class </li>
<li>dev set ΔF1 ΔAcc. p-value MaxEnt+ vs. MaxEnt 1.68 2.50 *0.054 MaxEnt+ vs. baseline 0.75 8.01 0.01 SVM+ vs. SVM 1.07 1.50 **0.21 SVM+ vs. baseline 4.25 11.52 0.01 test set ΔF1 ΔAcc. p-value MaxEnt+ vs MaxEnt 7.02 9.59 0.01 MaxEnt+ vs baseline 3.94 12.33 0.01 SVM+ vs SVM 3.13 4.79 0.02 SVM+ vs baseline 4.46 11.64 0.01 </li>
<li>The table 6 compares the two tasks. For the dev set, the difference between the two tasks for the same model doesn’t exhibit significant p-values see * and ** , especially for the SVM which seems to be able to learn easily without any additional information. On the test set, the feature predic- tOnto shows significant improvements, though with a lesser gain for the SVM. </li>
<li>The accuracy and F1 resulting from our linear and non- linear classifiers show a pretty good stability. That suggests a good trade off between the variance and the bias of our features, whichever the chosen algorithm is. The SVM shows a low precision, but a better recall, while inverse results appear for the MaxEnt. Thus we intend to further combine both models using approaches such as bagging or boosting. </li>
</ul>
<h3>7. CONCLUSION AND PERSPECTIVES </h3>
<p>In this paper we investigate a new way for extracting tax- onomic relations from text, exploiting non-linear enumera- tive structures which clearly express hierarchical relations and for which classical NLP tools are not suitable. A su- pervised machine learning approach has been implemented and evaluated, thanks to two algorithms a linear and a non linear ones that give substantially the same results, i.e. an F1 around 81.5%. The results are promising and we plan a large scale application as Sumida et al. [26] do when they extract hyponymy relations from Japanese Wikipedia pages using their layout. </p>
<p>Our work is currently extended in two main directions. First of all, in order to build a full taxonomy, we obviously have to learn the concepts, which are linked by the tax- onomic relation. Second, we plan to extend our machine learning approach in order to differentiate, within non-linear </p>
<p>ES, the ontological relations including the meronymic rela- tions and the non-hierarchical ones from the lexical rela- tions including synonymy, antonymy, etc. . These more precise relations could then be involved both in terminology or ontology building processes. </p>
<ul>
<li>Proceedings of the fifth annual workshop on Computational learning theory, pages 144–152. ACM, </li>
<li>1992. [5] S. Cederberg and D. Widdows. Using lsa and noun coordination information to improve the precision and recall of automatic hyponymy extraction. In </li>
<li>Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, </li>
<li>pages 111–118. Association for Computational Linguistics, 2003. [6] S. Chernov, T. Iofciu, W. Nejdl, and X. Zhou. Extracting semantics relationships between wikipedia categories. SemWiki, 206, 2006. [7] P. Cimiano, S. Handschuh, and S. Staab. Towards the </li>
<li>self-annotating web. In Proceedings of the 13th international conference on World Wide Web, pages </li>
<li>462–471. ACM, 2004. </li>
<li>[8] P. Cimiano, A. Hotho, and S. Staab. Learning concept hierarchies from text corpora using formal concept </li>
<li>analysis. J. Artif. Intell. Res. JAIR , 24 305–339, </li>
<li>2005. [9] D. Faure and C. N´edellec. A corpus-based conceptual clustering method for verb frames and ontology </li>
<li>acquisition. In LREC workshop on adapting lexical and corpus resources to sublanguages and applications, </li>
</ul>
<p>volume 707, page 30, 1998. [10] J. Fleiss, J. Nee, and J. Landis. Large sample variance of kappa in the case of different sets of raters. Psychological Bulletin, 86 5 974–977, 1979. [11] M. A. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th </p>
<h3>conference on Computational linguistics, volume 2, </h3>
<p>pages 539–545. Association for Computational Linguistics, 1992. [12] A. Herbelot and A. Copestake. Acquiring ontological relationships from wikipedia using rmrs. In ISWC </p>
<h3>2006 Workshop on Web Content Mining with Human Language Technologies, Athens, Georgia. Citeseer, </h3>
<p>2006. [13] E. H. Hovy and Y. Arens. Automatic generation of </p>
<ul>
<li>[16] M. Kamel, B. Rothenburger, and J.-P. Fauconnier. Identification de relations s´emantiques port´ees par les structures ´enum´eratives paradigmatiques. Revue </li>
<li>d’Intelligence Artificielle, Ing´enierie des </li>
<li>Connaissances, 2014. [17] J. R. Landis and G. G. Koch. The measurement of observer agreement for categorical data. biometrics, pages 159–174, 1977. [18] A. Lenci and G. Benotto. Identifying hypernyms in distributional semantic spaces. In Proceedings of the </li>
<li>First Joint Conference on Lexical and Computational Semantics-Volume 1 Proceedings of the main conference and the shared task, and Volume 2 Proceedings of the Sixth International Workshop on </li>
<li>Semantic Evaluation, pages 75–79. Association for Computational Linguistics, 2012. [19] W. Mann and S. Thompson. Rhetorical structure theory Toward a functional theory of text organization. Text, 8 3 243–281, 1988. [20] O. Medelyan, D. Milne, C. Legg, and I.-H. Witten. Mining meaning from wikipedia. International Journal of Human-Computer Studies, 67 9 716–754, 2009. [21] M. J. O’Connor and A. Das. Acquiring owl ontologies </li>
<li>from xml documents. In Proceedings of the sixth international conference on Knowledge capture, pages </li>
<li>17–24. ACM, 2011. </li>
<li>[22] R. Power, D. Scott, and N. Bouayad-Agha. Document structure. Computational Linguistics, 29 2 211–260, 2003. [23] D. Sanchez and A. Moreno. Web-scale taxonomy </li>
<li>learning. In Proceedings of Workshop on Extending and Learning Lexical Ontologies using Machine </li>
<li>Learning ICML 2005 , pages 53–60, Bonn, Germany, 2005. [24] R. Snow, D. Jurafsky, and A. Y. Ng. Learning syntactic patterns for automatic hypernym discovery. </li>
<li>Advances in Neural Information Processing Systems 17, 2004. </li>
<li>[25] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago a core of semantic knowledge. In Proceedings of the 16th </li>
<li>international conference on World Wide Web, pages </li>
<li>697–706. ACM, 2007. </li>
<li>[26] A. Sumida and K. Torisawa. Hacking wikipedia for hyponymy relation acquisition. In IJCNLP, volume 8, pages 883–888. Citeseer, 2008. [27] A. Urieli and L. Tanguy. L’apport du faisceau dans l’analyse syntaxique en d´ependances par transitions ´etudes de cas avec l’analyseur Talismane. In Actes de </li>
<li>la 20e conf´erence sur le Traitement Automatique des </li>
<li>Langues Naturelles TALN’2013 , pages 188–201, Les Sables d’Olonne, France, 2013. [28] J. V¨ olker, D. Vrandeˇci´c, Y. Sure, and A. Hotho. </li>
<li>Learning disjointness. In The Semantic Web Research </li>
<li>and Applications, pages 175–189. Springer, 2007. </li>
<li>Conference AAAI 1991 , Anaheim, CA, 1991. </li>
</body>
</html>
